import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

nlp = spacy.load('en_core_web_sm')

text = """
Natural Language Processing (NLP) is a sub-field of artificial intelligence (AI) 
focused on the interaction between computers and humans through language.
"""
tokens = word_tokenize(text)

stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

spacy_lemmatized = [token.lemma_ for token in nlp(" ".join(filtered_tokens))]

print("Original Text:", text)
print("Tokenized:", tokens)
print("Filtered (Stop-word Removal):", filtered_tokens)
print("Stemmed:", stemmed_tokens)
print("Lemmatized:", lemmatized_tokens)
print("SpaCy Lemmatized:", spacy_lemmatized)

sentences = [filtered_tokens]  # List of tokenized sentences

word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

vector = word2vec_model.wv['language']
print("Vector for 'language':", vector)

similar_words = word2vec_model.wv.most_similar('language')
print("Words similar to 'language':", similar_words)
